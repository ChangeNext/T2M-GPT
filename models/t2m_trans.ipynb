{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/workspace\")\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "\n",
    "# import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_trans as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "import models.pos_encoding as pos_encoding\n",
    "import clip\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'dataname': 't2m',\n",
    "    'batch_size': 16,\n",
    "    'fps': [20],\n",
    "    'seq_len': 64,\n",
    "    'total_iter': 100000,\n",
    "    'warm_up_iter': 1000,\n",
    "    'lr': 0.0001,\n",
    "    'lr_scheduler': [150000],\n",
    "    'gamma': 0.05,\n",
    "    'weight_decay': 1e-6,\n",
    "    'decay_option': 'all',\n",
    "    'optimizer': 'adamw',\n",
    "    'code_dim': 512,\n",
    "    'nb_code': 512,\n",
    "    'mu': 0.99,\n",
    "    'down_t': 2,\n",
    "    'stride_t': 2,\n",
    "    'width': 512,\n",
    "    'depth': 3,\n",
    "    'dilation_growth_rate': 3,\n",
    "    'output_emb_width': 512,\n",
    "    'vq_act': 'relu',\n",
    "    'block_size': 51,\n",
    "    'embed_dim_gpt': 1024,\n",
    "    'clip_dim': 512,\n",
    "    'num_layers': 9,\n",
    "    'n_head_gpt': 16,\n",
    "    'ff_rate': 4,\n",
    "    'drop_out_rate': 0.1,\n",
    "    'quantizer': 'ema_reset',\n",
    "    'quantbeta': 1.0,\n",
    "    'resume_pth': \"output_vqfinal/VQ-VAE/reset/net_last.pth\",\n",
    "    'resume_trans': None,\n",
    "    'out_dir': 'output_GPT_Final/',\n",
    "    'exp_name': 'exp_debug',\n",
    "    'vq_name': 'exp_debug',\n",
    "    'print_iter': 200,\n",
    "    'eval_iter': 10000,\n",
    "    'seed': 123,\n",
    "    'if_maxtest': False,\n",
    "    'pkeep': 0.5\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.out_dir = os.path.join(args.out_dir, f'{args.exp_name}')\n",
    "args.vq_dir= os.path.join(\"./dataset/KIT-ML\" if args.dataname == 'kit' else \"./dataset/HumanML3D\", f'{args.vq_name}')\n",
    "os.makedirs(args.out_dir, exist_ok = True)\n",
    "os.makedirs(args.vq_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalCrossConditionalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=512, block_size=16, n_head=8, drop_out_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 8 == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(drop_out_rate)\n",
    "        self.resid_drop = nn.Dropout(drop_out_rate)\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() \n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "\n",
    "        #mask 값이 0인 부분을 -inf로 채우기 \n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        #Scaled Dot-Product Attention을 계산\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        #(B,  T,  m_ebed_dim)\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=512, block_size=16, n_head=8, drop_out_rate=0.1, fc_rate=4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = CausalCrossConditionalSelfAttention(embed_dim, block_size, n_head, drop_out_rate)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, fc_rate * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(fc_rate * embed_dim, embed_dim),\n",
    "            nn.Dropout(drop_out_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossCondTransBase(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                num_vq=1024, \n",
    "                embed_dim=512,\n",
    "                clip_dim=512, \n",
    "                block_size=16, \n",
    "                num_layers=2, \n",
    "                n_head=8, \n",
    "                drop_out_rate=0.1, \n",
    "                fc_rate=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(num_vq + 2, embed_dim)\n",
    "        self.cond_emb = nn.Linear(clip_dim, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.drop = nn.Dropout(drop_out_rate)\n",
    "        # transformer block\n",
    "        self.blocks = nn.Sequential(*[Block(embed_dim, block_size, n_head, drop_out_rate, fc_rate) for _ in range(num_layers)])\n",
    "        self.pos_embed = pos_encoding.PositionEmbedding(block_size, embed_dim, 0.0, False)\n",
    "\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, idx, clip_feature):\n",
    "        if len(idx) == 0:\n",
    "            token_embeddings = self.cond_emb(clip_feature).unsqueeze(1)\n",
    "        else:\n",
    "            print(idx.size())\n",
    "            b, t = idx.size()\n",
    "            assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "            # forward the Trans model\n",
    "            #idx = motion_idx, feat_clip_text=>()\n",
    "            token_embeddings = self.tok_emb(idx)\n",
    "            print(\"idx size :\" , idx.shape)\n",
    "            print(\"token ebedding : \", token_embeddings.shape)\n",
    "            print(\"cond_emd shape\", self.cond_emb(clip_feature).unsqueeze(1).shape)\n",
    "            token_embeddings = torch.cat([self.cond_emb(clip_feature).unsqueeze(1), token_embeddings], dim=1)\n",
    "            print(\"token ebedding_cat : \", token_embeddings.shape)\n",
    "        \n",
    "        x = self.pos_embed(token_embeddings)\n",
    "        print(\"pos\",x.shape)\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        print(\"block\",x.shape)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossCondTransHead(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                num_vq=1024, \n",
    "                embed_dim=512, \n",
    "                block_size=16, \n",
    "                num_layers=2, \n",
    "                n_head=8, \n",
    "                drop_out_rate=0.1, \n",
    "                fc_rate=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(embed_dim, block_size, n_head, drop_out_rate, fc_rate) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_vq + 1, bias=False)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        print(\"x : \", x.shape)\n",
    "        x = self.ln_f(x)\n",
    "        print(\"x_layerNorm : \", x.shape)\n",
    "        logits = self.head(x)\n",
    "        print(\"logits : \", logits.shape)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2Motion_Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                num_vq=1024, \n",
    "                embed_dim=512, \n",
    "                clip_dim=512, \n",
    "                block_size=16, \n",
    "                num_layers=2, \n",
    "                n_head=8, \n",
    "                drop_out_rate=0.1, \n",
    "                fc_rate=4):\n",
    "        super().__init__()\n",
    "        self.trans_base = CrossCondTransBase(num_vq, embed_dim, clip_dim, block_size, num_layers, n_head, drop_out_rate, fc_rate)\n",
    "        self.trans_head = CrossCondTransHead(num_vq, embed_dim, block_size, num_layers, n_head, drop_out_rate, fc_rate)\n",
    "        self.block_size = block_size\n",
    "        self.num_vq = num_vq\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idxs, clip_feature):\n",
    "        feat = self.trans_base(idxs, clip_feature)\n",
    "        logits = self.trans_head(feat)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, clip_feature, if_categorial=False):\n",
    "        for k in range(self.block_size):\n",
    "            if k == 0:\n",
    "                x = []\n",
    "            else:\n",
    "                x = xs\n",
    "            logits = self.forward(x, clip_feature)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if if_categorial:\n",
    "                dist = Categorical(probs)\n",
    "                idx = dist.sample()\n",
    "                if idx == self.num_vq:\n",
    "                    break\n",
    "                idx = idx.unsqueeze(-1)\n",
    "            else:\n",
    "                _, idx = torch.topk(probs, k=1, dim=-1)\n",
    "                if idx[0] == self.num_vq:\n",
    "                    break\n",
    "            # append to the sequence and continue\n",
    "            if k == 0:\n",
    "                xs = idx\n",
    "            else:\n",
    "                xs = torch.cat((xs, idx), dim=1)\n",
    "            \n",
    "            if k == self.block_size - 1:\n",
    "                return xs[:, :-1]\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text2Motion_Transformer(\n",
       "  (trans_base): CrossCondTransBase(\n",
       "    (tok_emb): Embedding(514, 1024)\n",
       "    (cond_emb): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (pos_embedding): Embedding(51, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_embed): PositionEmbedding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (trans_head): CrossCondTransHead(\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=1024, out_features=513, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_encoder = Text2Motion_Transformer(num_vq=args.nb_code, \n",
    "                                embed_dim=args.embed_dim_gpt, \n",
    "                                clip_dim=args.clip_dim, \n",
    "                                block_size=args.block_size, \n",
    "                                num_layers=args.num_layers, \n",
    "                                n_head=args.n_head_gpt, \n",
    "                                drop_out_rate=args.drop_out_rate, \n",
    "                                fc_rate=args.ff_rate)\n",
    "\n",
    "trans_encoder.train()\n",
    "trans_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:03<00:00, 6704.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:03<00:00, 7469.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader_token = dataset_tokenize.DATALoader(args.dataname, 1, unit_length=2**args.down_t)\n",
    "\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, args.vq_name, unit_length=2**args.down_t)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 51])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_text, m_tokens, m_tokens_len = batch\n",
    "m_tokens.shape\n",
    "m_tokens, m_tokens_len = m_tokens.cuda(), m_tokens_len.cuda()\n",
    "bs = m_tokens.shape[0]\n",
    "m_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 51])\n",
      "tensor([[49406,   518,  2533,  ...,     0,     0,     0],\n",
      "        [49406,   518,  5706,  ...,     0,     0,     0],\n",
      "        [49406,   320,  2533,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [49406,   320,  2533,  ...,     0,     0,     0],\n",
      "        [49406,   320,  2533,  ...,     0,     0,     0],\n",
      "        [49406,   518,  2533,  ...,     0,     0,     0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "torch.Size([16, 77])\n"
     ]
    }
   ],
   "source": [
    "target = m_tokens    # (bs, 51)\n",
    "print(target.shape)\n",
    "target = target.cuda()\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "print(text)\n",
    "print(text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50])\n",
      "tensor([[390, 144, 282, 301, 452, 260, 102, 294, 480, 233, 267, 275,  12, 419,\n",
      "         357, 135, 452, 203,  89, 330, 113, 409, 194, 438, 384, 412, 321, 321,\n",
      "          52,  71, 257, 472, 324, 111, 381, 267, 113, 324, 159, 219, 393, 468,\n",
      "         471, 234, 275, 412, 393, 310, 316, 512],\n",
      "        [165, 175, 224, 319, 319, 319, 319, 319, 224, 224,  31, 473, 473, 473,\n",
      "         291, 405, 354, 175, 224,  31,  37, 291, 291, 405, 354, 175, 224, 224,\n",
      "          37, 473, 473, 473, 473, 473,  31, 175, 175, 475,  17, 319, 319, 319,\n",
      "         319, 319, 319, 319, 319, 224, 165, 512],\n",
      "        [402, 402, 402, 402, 127, 328, 319,  53, 449, 258,  52, 131, 248, 489,\n",
      "          11, 185, 454,  33, 206,  97,  97,  97, 512, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [273, 273, 317, 168, 168, 273, 317, 159, 328, 254, 300, 282, 220, 350,\n",
      "         453,  11,  28, 318, 184, 448, 228,  40,  55,  61, 444, 150, 209, 332,\n",
      "         371, 488, 318,  94, 448, 471, 220, 103, 453, 448, 404, 365, 112, 244,\n",
      "         146, 473, 163,  89, 508, 437, 512, 513],\n",
      "        [447,  95, 212, 313, 246, 100, 100, 336, 413, 413, 291, 473,  37,  56,\n",
      "         337, 212, 313,  97, 100, 100,  16,  16, 413, 473, 291, 420, 447, 212,\n",
      "         313, 254, 246, 100, 100, 431, 413, 397, 297,  67, 100,  56, 311, 313,\n",
      "         512, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [ 17,  17, 159, 328, 206, 236, 371, 179, 176, 328, 365, 206, 258, 139,\n",
      "         258, 475, 318, 275, 236, 236, 475, 496, 337, 337, 481, 475, 475, 176,\n",
      "         512, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [487, 487,  17,  17,  17,  17, 402,  17, 467, 501, 511, 321, 429, 430,\n",
      "         106, 203, 294, 116, 369,  62, 106, 303, 493, 308, 481, 402, 402, 487,\n",
      "          17,  17,  17,  17,  17,  17, 176, 512, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [319, 319, 319, 127, 328, 125, 294, 444, 147, 509, 318, 330, 185, 235,\n",
      "         147, 111, 275, 319, 319, 319, 319, 512, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [435, 435, 435, 435, 435, 304, 471, 357,  32, 350, 401,  56, 235, 462,\n",
      "         407,  33, 178, 282, 262,  25, 460, 512, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [134, 134, 408,  69, 470,  87, 201, 201, 201, 201, 384, 224, 176, 461,\n",
      "         429, 425,  81, 144, 193, 193, 512, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [487, 348, 337, 222, 211, 222, 222, 331, 354, 184, 487, 224, 413, 354,\n",
      "         222, 211, 211, 211, 405, 512, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [399, 404, 404, 310, 258,  83, 350, 388, 109, 371, 418,  83, 318, 248,\n",
      "         129, 179, 404, 313, 313, 506, 358, 458, 399, 478, 457,  55, 292,  49,\n",
      "         287, 150,  33, 332,  14,  14,  68, 343,  97, 404,  97, 358, 282, 346,\n",
      "         399, 312, 111, 421, 371, 146, 512, 513],\n",
      "        [487, 383, 383, 383, 383, 383, 383, 383, 409, 357,  17, 127, 328, 275,\n",
      "          17, 282, 179, 365, 275, 409, 199, 415,  32, 328, 275,  89, 294, 147,\n",
      "         236,  17,  17, 383, 383, 383, 383, 383, 383, 512, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [279, 346,  24, 146, 296, 256, 168, 179, 471,  24,  24, 294,  78, 509,\n",
      "         337, 496, 394, 337, 337, 337,  24,  24,  24,  24,  24,  24,  24,  24,\n",
      "         113, 185, 146, 394, 282, 462, 256, 294, 468, 146,  24, 113, 415, 163,\n",
      "         466, 147, 162,  24,  24, 294, 512, 513],\n",
      "        [111, 154, 330, 489, 449, 147, 465, 318,  81, 381, 371, 178, 178,  25,\n",
      "         460, 437, 437, 437, 437,  25, 512, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513],\n",
      "        [352, 352, 352, 153,  17, 294, 199,  63, 176, 289,  33, 330,  16,  78,\n",
      "         179, 199, 415,  89, 275, 401, 275, 236, 509, 162, 418, 475, 352,  95,\n",
      "          96,  17, 512, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513,\n",
      "         513, 513, 513, 513, 513, 513, 513, 513]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "feat_clip_text = clip_model.encode_text(text).float()\n",
    "input_index = target[:,:-1]\n",
    "print(input_index.shape)\n",
    "print(input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 베르누이 분포에서 난수 0,1로 그림\n",
    "#([128, 50])\n",
    "mask = torch.bernoulli(args.pkeep * torch.ones(input_index.shape, device=input_index.device))\n",
    "mask = mask.round().to(dtype=torch.int64)\n",
    "r_indices = torch.randint_like(input_index, args.nb_code)\n",
    "print(r_indices.shape)\n",
    "#mask 값이 1인 위치에는 input_index의 값을 넣고 0에서는 r_indices\n",
    "#노이즈를 추가하는 효과\n",
    "a_indices = mask*input_index+(1-mask)*r_indices\n",
    "a_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50])\n",
      "idx size : torch.Size([16, 50])\n",
      "token ebedding :  torch.Size([16, 50, 1024])\n",
      "cond_emd shape torch.Size([16, 1, 1024])\n",
      "token ebedding_cat :  torch.Size([16, 51, 1024])\n",
      "pos torch.Size([16, 51, 1024])\n",
      "block torch.Size([16, 51, 1024])\n",
      "x :  torch.Size([16, 51, 1024])\n",
      "x_layerNorm :  torch.Size([16, 51, 1024])\n",
      "logits :  torch.Size([16, 51, 513])\n",
      "torch.Size([16, 51, 513])\n"
     ]
    }
   ],
   "source": [
    "cls_pred = trans_encoder(a_indices, feat_clip_text)   \n",
    "print(cls_pred.shape) \n",
    "cls_pred = cls_pred.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 51, 513])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cls = 0.0\n",
    "loss_cls += loss_ce(cls_pred[0][:m_tokens_len[0] + 1], target[0][:m_tokens_len[0] + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_tokens_len[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 513])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred[2][:m_tokens_len[2] + 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.5367, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 513])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred[0][:m_tokens_len[0] + 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 513])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(cls_pred[0][:m_tokens_len[0] + 1], dim=-1)\n",
    "\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Categorical(probs)\n",
    "cls_pred_index = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls_pred_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_num = 0\n",
    "right_num += (cls_pred_index.flatten(0) == target[0][:m_tokens_len[0] + 1].flatten(0)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([390, 144, 282, 301, 452, 260, 102, 294, 480, 233, 267, 275,  12, 419,\n",
       "        357, 135, 452, 203,  89, 330, 113, 409, 194, 438, 384, 412, 321, 321,\n",
       "         52,  71, 257, 472, 324, 111, 381, 267, 113, 324, 159, 219, 393, 468,\n",
       "        471, 234, 275, 412, 393, 310, 316, 512], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0][:m_tokens_len[0] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cls_pred_index = torch.max(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([449, 357,  41, 233,  89,  41, 202, 476, 476, 233, 233, 229,  15,  41,\n",
       "         50,  41,  50,  50,  15,  50,  31,  50,  50, 181, 229,  50, 476, 220,\n",
       "         68,  50,  50,  50,  50, 229,  15,  15,  98,  15, 476, 476, 110,  50,\n",
       "         50,  50,  50,  50, 476,  50, 304, 466], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 23,  96,  54, 378, 237, 441, 141,  30, 272, 461, 461,  75, 339, 355,\n",
       "        338, 288, 229, 283, 231,  34,  56,  50, 442,  78, 184,  10, 306, 181,\n",
       "        167, 142, 180, 375, 372, 132, 371, 361, 261, 509,  27, 168, 334, 266,\n",
       "        460, 332, 142, 294, 382, 378, 233, 466], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred_index.flatten(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([390, 144, 282, 301, 452, 260, 102, 294, 480, 233, 267, 275,  12, 419,\n",
       "        357, 135, 452, 203,  89, 330, 113, 409, 194, 438, 384, 412, 321, 321,\n",
       "         52,  71, 257, 472, 324, 111, 381, 267, 113, 324, 159, 219, 393, 468,\n",
       "        471, 234, 275, 412, 393, 310, 316, 512], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0][:m_tokens_len[0] + 1].flatten(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "probs = torch.tensor([0.2, 0.3, 0.5])  # 각 범주에 대한 확률 값\n",
    "dist = torch.distributions.Categorical(probs)  # Categorical 분포 객체 생성\n",
    "\n",
    "sample = dist.sample()  # 분포를 따라 샘플링\n",
    "print(sample)  # 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
    "y = m.sample()  # equal probability of 0, 1, 2, 3\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
