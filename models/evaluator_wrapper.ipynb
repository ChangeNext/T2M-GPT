{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models.evaluator_wrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6257/447741456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vectorizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPOS_enumerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_eval_option\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluatorModelWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models.evaluator_wrapper'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from os.path import join as pjoin\n",
    "import os \n",
    "os.chdir(\"/workspace\")\n",
    "import numpy as np\n",
    "from models.modules import MovementConvEncoder, TextEncoderBiGRUCo, MotionEncoderBiGRUCo\n",
    "from utils.word_vectorizer import POS_enumerator\n",
    "from options.get_eval_option import get_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "opt = easydict.EasyDict({\n",
    "    \"dataname\": \"t2m\",\n",
    "    \"batch_size\": 256,\n",
    "    \"window_size\": 64,\n",
    "    \"total_iter\": 300000,\n",
    "    \"warm_up_iter\": 1000,\n",
    "    \"lr\": 2e-4,\n",
    "    \"lr_scheduler\": [50000, 400000],\n",
    "    \"gamma\": 0.05,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"commit\": 0.02,\n",
    "    \"loss_vel\": 0.1,\n",
    "    \"recons_loss\": \"l1\",\n",
    "    \"code_dim\": 512,\n",
    "    \"nb_code\": 512,\n",
    "    \"mu\": 0.99,\n",
    "    \"down_t\": 2,\n",
    "    \"stride_t\": 2,\n",
    "    \"width\": 512,\n",
    "    \"depth\": 3,\n",
    "    \"dilation_growth_rate\": 3,\n",
    "    \"output_emb_width\": 512,\n",
    "    \"vq_act\": \"relu\",\n",
    "    \"vq_norm\": None,\n",
    "    \"quantizer\": \"orig\",\n",
    "    \"beta\": 1.0,\n",
    "    \"resume_pth\": None,\n",
    "    \"resume_gpt\": None,\n",
    "    \"out_dir\": \"output_vqfinal/\",\n",
    "    \"results_dir\": \"visual_results/\",\n",
    "    \"visual_name\": \"baseline\",\n",
    "    \"exp_name\": \"VQ-VAE\",\n",
    "    \"print_iter\": 200,\n",
    "    \"eval_iter\": 1000,\n",
    "    \"seed\": 123,\n",
    "    \"vis_gt\": False,\n",
    "    \"nb_vis\": 20,\n",
    "    \"nb_joints\" : 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(opt):\n",
    "    movement_enc = MovementConvEncoder(opt.dim_pose-4, opt.dim_movement_enc_hidden, opt.dim_movement_latent)\n",
    "    text_enc = TextEncoderBiGRUCo(word_size=opt.dim_word,\n",
    "                                  pos_size=opt.dim_pos_ohot,\n",
    "                                  hidden_size=opt.dim_text_hidden,\n",
    "                                  output_size=opt.dim_coemb_hidden,\n",
    "                                  device=opt.device)\n",
    "\n",
    "    motion_enc = MotionEncoderBiGRUCo(input_size=opt.dim_movement_latent,\n",
    "                                      hidden_size=opt.dim_motion_hidden,\n",
    "                                      output_size=opt.dim_coemb_hidden,\n",
    "                                      device=opt.device)\n",
    "    checkpoint = torch.load(pjoin(opt.checkpoints_dir, opt.dataset_name, 'text_mot_match', 'model', 'finest.tar'),\n",
    "                            map_location=opt.device)\n",
    "    movement_enc.load_state_dict(checkpoint['movement_encoder'])\n",
    "    text_enc.load_state_dict(checkpoint['text_encoder'])\n",
    "    motion_enc.load_state_dict(checkpoint['motion_encoder'])\n",
    "    print('Loading Evaluation Model Wrapper (Epoch %d) Completed!!' % (checkpoint['epoch']))\n",
    "    \n",
    "    return text_enc, motion_enc, movement_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorModelWrapper(object):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "\n",
    "        if opt.dataset_name == 't2m':\n",
    "            opt.dim_pose = 263\n",
    "        elif opt.dataset_name == 'kit':\n",
    "            opt.dim_pose = 251\n",
    "        else:\n",
    "            raise KeyError('Dataset not Recognized!!!')\n",
    "\n",
    "        opt.dim_word = 300\n",
    "        opt.max_motion_length = 196\n",
    "        opt.dim_pos_ohot = len(POS_enumerator)\n",
    "        opt.dim_motion_hidden = 1024\n",
    "        opt.max_text_len = 20\n",
    "        opt.dim_text_hidden = 512\n",
    "        opt.dim_coemb_hidden = 512\n",
    "\n",
    "        # print(opt)\n",
    "\n",
    "        self.text_encoder, self.motion_encoder, self.movement_encoder = build_models(opt)\n",
    "        self.opt = opt\n",
    "        self.device = opt.device\n",
    "\n",
    "        self.text_encoder.to(opt.device)\n",
    "        self.motion_encoder.to(opt.device)\n",
    "        self.movement_encoder.to(opt.device)\n",
    "\n",
    "        self.text_encoder.eval()\n",
    "        self.motion_encoder.eval()\n",
    "        self.movement_encoder.eval()\n",
    "\n",
    "    # Please note that the results does not following the order of inputs\n",
    "    def get_co_embeddings(self, word_embs, pos_ohot, cap_lens, motions, m_lens):\n",
    "        with torch.no_grad():\n",
    "            word_embs = word_embs.detach().to(self.device).float()\n",
    "            pos_ohot = pos_ohot.detach().to(self.device).float()\n",
    "            motions = motions.detach().to(self.device).float()\n",
    "\n",
    "            '''Movement Encoding'''\n",
    "            movements = self.movement_encoder(motions[..., :-4]).detach()\n",
    "            m_lens = m_lens // self.opt.unit_length\n",
    "            motion_embedding = self.motion_encoder(movements, m_lens)\n",
    "\n",
    "            '''Text Encoding'''\n",
    "            text_embedding = self.text_encoder(word_embs, pos_ohot, cap_lens)\n",
    "        return text_embedding, motion_embedding\n",
    "\n",
    "    # Please note that the results does not following the order of inputs\n",
    "    def get_motion_embeddings(self, motions, m_lens):\n",
    "        with torch.no_grad():\n",
    "            motions = motions.detach().to(self.device).float()\n",
    "\n",
    "            align_idx = np.argsort(m_lens.data.tolist())[::-1].copy()\n",
    "            motions = motions[align_idx]\n",
    "            m_lens = m_lens[align_idx]\n",
    "\n",
    "            '''Movement Encoding'''\n",
    "            movements = self.movement_encoder(motions[..., :-4]).detach()\n",
    "            m_lens = m_lens // self.opt.unit_length\n",
    "            motion_embedding = self.motion_encoder(movements, m_lens)\n",
    "        return motion_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "dataset_opt_path = 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "nb_joints = 22\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "a, b, c = build_models(wrapper_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextEncoderBiGRUCo(\n",
       "  (pos_emb): Linear(in_features=15, out_features=300, bias=True)\n",
       "  (input_emb): Linear(in_features=300, out_features=512, bias=True)\n",
       "  (gru): GRU(512, 512, batch_first=True, bidirectional=True)\n",
       "  (output_net): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
