{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# sys.argv = ['GPT_eval_multi.py']\n",
    "os.chdir(\"/workspace/\")\n",
    "# import options.option_transformer as option_trans\n",
    "# args = option_trans.get_args_parser()\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import models.vqvae as vqvae\n",
    "import models.t2m_trans as trans\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "from lion_pytorch import Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'dataname': 't2m',\n",
    "    'batch_size': 64,\n",
    "    'fps': [20],\n",
    "    'seq_len': 64,\n",
    "    'total_iter': 300000,\n",
    "    'warm_up_iter': 1000,\n",
    "    'lr': 0.0001,\n",
    "    'lr_scheduler': [150000],\n",
    "    'gamma': 0.05,\n",
    "    'weight_decay': 1e-6,\n",
    "    'decay_option': 'all',\n",
    "    'optimizer': 'adamw',\n",
    "    'code_dim': 512,\n",
    "    'nb_code': 512,\n",
    "    'mu': 0.99,\n",
    "    'down_t': 2,\n",
    "    'stride_t': 2,\n",
    "    'width': 512,\n",
    "    'depth': 3,\n",
    "    'dilation_growth_rate': 3,\n",
    "    'output_emb_width': 512,\n",
    "    'vq_act': 'relu',\n",
    "    'block_size': 51,\n",
    "    'embed_dim_gpt': 1024,\n",
    "    'clip_dim': 512,\n",
    "    'num_layers': 9,\n",
    "    'n_head_gpt': 16,\n",
    "    'ff_rate': 4,\n",
    "    'drop_out_rate': 0.1,\n",
    "    'quantizer': 'ema_reset',\n",
    "    'quantbeta': 1.0,\n",
    "    'resume_pth': \"output_vqfinal/VQ-VAE/eval/net_last.pth\",\n",
    "    'resume_trans': \"output_GPT_Final/pkeep_0.5/net_best_fid.pth\",\n",
    "    'resume_trans2': \"output_GPT_Final/pkeep_12layer_0.5/net_best_fid.pth\",\n",
    "    'out_dir': 'output_GPT_Final/',\n",
    "    'exp_name': 'pkeep_0.5/',\n",
    "    'vq_name': 'VQ-VAE',\n",
    "    'print_iter': 200,\n",
    "    'eval_iter': 10000,\n",
    "    'seed': 123,\n",
    "    'if_maxtest': False,\n",
    "    'pkeep': 0.5\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load clip model and datasets\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False, download_root='./')  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint from output_vqfinal/VQ-VAE/eval/net_last.pth\n",
      "loading transformer checkpoint from output_GPT_Final/pkeep_0.5/net_best_fid.pth\n"
     ]
    }
   ],
   "source": [
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate)\n",
    "\n",
    "\n",
    "trans_encoder = trans.Text2Motion_Transformer(num_vq=args.nb_code, \n",
    "                                embed_dim=1024, \n",
    "                                clip_dim=args.clip_dim, \n",
    "                                block_size=args.block_size, \n",
    "                                num_layers=9, \n",
    "                                n_head=16, \n",
    "                                drop_out_rate=args.drop_out_rate, \n",
    "                                fc_rate=args.ff_rate)\n",
    "\n",
    "\n",
    "print ('loading checkpoint from {}'.format(args.resume_pth))\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "\n",
    "print ('loading transformer checkpoint from {}'.format(args.resume_trans))\n",
    "ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
    "trans_encoder.eval()\n",
    "trans_encoder.cuda()\n",
    "\n",
    "mean = torch.from_numpy(np.load('./checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy')).cuda()\n",
    "std = torch.from_numpy(np.load('./checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy')).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading transformer checkpoint from output_GPT_Final/pkeep_12layer_0.5/net_best_fid.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text2Motion_Transformer(\n",
       "  (trans_base): CrossCondTransBase(\n",
       "    (tok_emb): Embedding(514, 1024)\n",
       "    (cond_emb): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (pos_embedding): Embedding(51, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_embed): PositionEmbedding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (trans_head): CrossCondTransHead(\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=1024, out_features=513, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_encoder2 = trans.Text2Motion_Transformer(num_vq=args.nb_code, \n",
    "                                embed_dim=1024, \n",
    "                                clip_dim=args.clip_dim, \n",
    "                                block_size=args.block_size, \n",
    "                                num_layers=12, \n",
    "                                n_head=16, \n",
    "                                drop_out_rate=args.drop_out_rate, \n",
    "                                fc_rate=args.ff_rate)\n",
    "\n",
    "print ('loading transformer checkpoint from {}'.format(args.resume_trans2))\n",
    "ckpt1 = torch.load(args.resume_trans2, map_location='cpu')\n",
    "trans_encoder2.load_state_dict(ckpt1['trans'], strict=True)\n",
    "trans_encoder2.eval()\n",
    "trans_encoder2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualization.plot_3d_global as plot_3d\n",
    "\n",
    "\n",
    "def generate_motion(clip_text, transdecoder, num_layer, i = None, path = None):\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "     \n",
    "    text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "    feat_clip_text = clip_model.encode_text(text).float()\n",
    "    index_motion = transdecoder.sample(feat_clip_text[0:1], False)\n",
    "    pred_pose = net.forward_decoder(index_motion)\n",
    "    pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
    "    xyz = pred_xyz.reshape(1, -1, 22, 3)\n",
    "    if i is None:\n",
    "        if num_layer == 9:\n",
    "            # np.save(f'./test_9layer/motion_{clip_text}.npy', xyz.detach().cpu().numpy())\n",
    "            pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),clip_text, [f'{num_layer}_{clip_text}.gif'], path)\n",
    "        elif num_layer == 12:\n",
    "            # np.save(f'./test_12layer/motion_{clip_text}.npy', xyz.detach().cpu().numpy())\n",
    "            pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),clip_text, [f'{num_layer}_{clip_text}.gif'], path)\n",
    "    else:\n",
    "        if num_layer == 9:\n",
    "            # np.save(f'./test_9layer/motion_{clip_text}.npy', xyz.detach().cpu().numpy())\n",
    "            pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),clip_text, [f'{num_layer}_{clip_text}_{i}.gif'], path)\n",
    "        elif num_layer == 12:\n",
    "            # np.save(f'./test_12layer/motion_{clip_text}.npy', xyz.detach().cpu().numpy())\n",
    "            pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),clip_text, [f'{num_layer}_{clip_text}_{i}.gif'], path)\n",
    "\n",
    "def  gt_motion(name, path = None):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    gt = np.load(f\"./dataset/HumanML3D/new_joints/{name}.npy\")\n",
    "    gt = gt.reshape(1, -1, 22, 3)\n",
    "    data = pd.read_csv(\"test_gt.csv\")\n",
    "    data.drop(columns=\"Unnamed: 0\")\n",
    "    text = data[data['name'] == str(name)].text\n",
    "    clip_text = list(text)\n",
    "    pose_vis = plot_3d.draw_to_batch(gt, clip_text, [f'{name}_{clip_text}_gt.gif'], path)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"test_gt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_text= ['a person raises both hands to their chest, rotates their torso, squats in this position then lowers their left hand and rotates their torso again. now lowering their hand']\n",
    "dir = \"need\"\n",
    "generate_motion(clip_text, trans_encoder, 9, None,path= \"./test_9layer/{dir}/\")\n",
    "# gt_motion(\"010167\", f\"./test_gt/{dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133    003812\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "clip_text= ['a person steps forward and shakes both hands together in a begging manner, and then steps back. the person steps forward again and shakes both hands in a begging manner but more aggressively.']\n",
    "name = df[df['text']== clip_text[0]][\"name\"]\n",
    "print(name)\n",
    "dir = \"motondiffuse\"\n",
    "generate_motion(clip_text, trans_encoder, 9, None,path= f\"./test_9layer/{dir}/\")\n",
    "# generate_motion(clip_text, trans_encoder2, 12,None,path=\"./test_12layer/arm/\")\n",
    "gt_motion(str(list(name)[0]), f\"./test_gt/{dir}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은말이지만 다양한 의미인 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person takes a step forward with left leg and then raise the right leg.']\n",
      "['person takes a step forward with left leg and then put up the right leg.']\n"
     ]
    }
   ],
   "source": [
    "clip_text= [[\"person takes a step forward with left leg and then raise the right leg.\"],\n",
    "            [\"person takes a step forward with left leg and then put up the right leg.\"]]\n",
    "\n",
    "for i, clip_text in enumerate(clip_text):\n",
    "    print(clip_text)\n",
    "    generate_motion(clip_text, trans_encoder, 9, i,path=\"./test_9layer/a/\")\n",
    "    # generate_motion(clip_text, trans_encoder2, 12,i,path=\"./test_12layer/a/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상함\n",
    "\"M004545\"\n",
    "a person jumping while rasing both hands, and  move apart legs\n",
    "person does squat thrusts three times with legs going outwards and arms going above head\n",
    "a person jumps with legs open while clapping with hands over head simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import base64\n",
    "from utils.motion_process import recover_from_ric\n",
    "\n",
    "# change the text here\n",
    "clip_text = [\"a man flying\"]\n",
    "\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "feat_clip_text = clip_model.encode_text(text).float()\n",
    "index_motion = trans_encoder.sample(feat_clip_text[0:1], False)\n",
    "index_motion2 = trans_encoder2.sample(feat_clip_text[0:1], False)\n",
    "pred_pose = net.forward_decoder(index_motion)\n",
    "pred_pose2 = net.forward_decoder(index_motion2)\n",
    "\n",
    "pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
    "pred_xyz2 = recover_from_ric((pred_pose2*std+mean).float(), 22)\n",
    "xyz = pred_xyz.reshape(1, -1, 22, 3)\n",
    "xyz2 = pred_xyz2.reshape(1, -1, 22, 3)\n",
    "\n",
    "np.save('motion_9layer.npy', xyz.detach().cpu().numpy())\n",
    "np.save('motion_12layer.npy', xyz2.detach().cpu().numpy())\n",
    "\n",
    "import visualization.plot_3d_global as plot_3d\n",
    "pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),clip_text, ['example.gif'])\n",
    "pose_vis = plot_3d.draw_to_batch(xyz2.detach().cpu().numpy(),clip_text, ['example2.gif'])\n",
    "\n",
    "\n",
    "\n",
    "b64 = base64.b64encode(open('example.gif','rb').read()).decode('ascii')\n",
    "display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST DATA 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:03<00:00, 1185.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "from dataset import dataset_TM_eval\n",
    "\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, True, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "name_text = []\n",
    "for batch in val_loader:\n",
    "    word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token, name = batch\n",
    "    name_text.append(list(name))\n",
    "    test_text.append(list(clip_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_text))\n",
    "type(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "text_list = []\n",
    "for i in range(144):\n",
    "    for name,text in zip(name_text[i],test_text[i]):\n",
    "        name_list.append(str(name))\n",
    "        text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"name\" : name_list, \"text\" : text_list})\n",
    "df.to_csv(\"./test_gt.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './test.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    for i in range(145):\n",
    "        for num, i in enumerate(test_text[i]):\n",
    "            file.write(''.join(i) + \"\\n\")\n",
    "            #else:\n",
    "             #   file.write(''.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
