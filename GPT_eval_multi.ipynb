{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/workspace/\")\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import clip\n",
    "\n",
    "# import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_eval\n",
    "import models.t2m_trans as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'dataname': 't2m',\n",
    "    'batch_size': 128,\n",
    "    'fps': [20],\n",
    "    'seq_len': 64,\n",
    "    'total_iter': 300000,\n",
    "    'warm_up_iter': 1000,\n",
    "    'lr': 0.0001,\n",
    "    'lr_scheduler': [150000],\n",
    "    'gamma': 0.05,\n",
    "    'weight_decay': 1e-6,\n",
    "    'decay_option': 'all',\n",
    "    'optimizer': 'adamw',\n",
    "    'code_dim': 512,\n",
    "    'nb_code': 512,\n",
    "    'mu': 0.99,\n",
    "    'down_t': 2,\n",
    "    'stride_t': 2,\n",
    "    'width': 512,\n",
    "    'depth': 3,\n",
    "    'dilation_growth_rate': 3,\n",
    "    'output_emb_width': 512,\n",
    "    'vq_act': 'relu',\n",
    "    'block_size': 51,\n",
    "    'embed_dim_gpt': 1024,\n",
    "    'clip_dim': 512,\n",
    "    'num_layers': 12,\n",
    "    'n_head_gpt': 16,\n",
    "    'ff_rate': 4,\n",
    "    'drop_out_rate': 0.1,\n",
    "    'quantizer': 'ema_reset',\n",
    "    'quantbeta': 1.0,\n",
    "    'resume_pth': \"output_vqfinal/VQ-VAE/eval/net_last.pth\",\n",
    "    'out_dir': 'output_GPT_Final/',\n",
    "    'exp_name': 't_t_12layer_pkeep_0.5',\n",
    "    'vq_name': 'VQ-VAE',\n",
    "    'print_iter': 200,\n",
    "    'eval_iter': 10000,\n",
    "    'seed': 123,\n",
    "    'if_maxtest': False,\n",
    "    'pkeep': 0.5,\n",
    "    'resume_trans' : 'output_GPT_Final/pkeep_12layer_0.5/net_best_fid.pth'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output_GPT_Final/pkeep_12layer_0.5/net_best_fid.pth'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Exp dirs ---- #####\n",
    "# args = option_vq.get_args_parser()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.out_dir = os.path.join(args.out_dir, f'{args.exp_name}')\n",
    "os.makedirs(args.out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output_GPT_Final/t_t_12layer_pkeep_0.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Logger ---- #####\n",
    "logger = utils_model.get_logger(args.out_dir)\n",
    "writer = SummaryWriter(args.out_dir)\n",
    "# logger.info(json.dumps(vars(args), indent=4, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:03<00:00, 1241.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, True, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "dataset_opt_path = 'checkpoints/kit/Comp_v6_KLD005/opt.txt' if args.dataname == 'kit' else 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load clip model and datasets\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate)\n",
    "\n",
    "\n",
    "trans_encoder = trans.Text2Motion_Transformer(num_vq=args.nb_code, \n",
    "                                embed_dim=args.embed_dim_gpt, \n",
    "                                clip_dim=args.clip_dim, \n",
    "                                block_size=args.block_size, \n",
    "                                num_layers=args.num_layers, \n",
    "                                n_head=args.n_head_gpt, \n",
    "                                drop_out_rate=args.drop_out_rate, \n",
    "                                fc_rate=args.ff_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint from output_vqfinal/VQ-VAE/eval/net_last.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HumanVQVAE(\n",
       "  (vqvae): VQVAE_251(\n",
       "    (encoder): Encoder(\n",
       "      (model): Sequential(\n",
       "        (0): Conv1d(263, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (1): Resnet1D(\n",
       "            (model): Sequential(\n",
       "              (0): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (2): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (1): Resnet1D(\n",
       "            (model): Sequential(\n",
       "              (0): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (2): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (model): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Resnet1D(\n",
       "            (model): Sequential(\n",
       "              (0): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (2): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Resnet1D(\n",
       "            (model): Sequential(\n",
       "              (0): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (2): ResConv1DBlock(\n",
       "                (norm1): Identity()\n",
       "                (norm2): Identity()\n",
       "                (activation1): ReLU()\n",
       "                (activation2): ReLU()\n",
       "                (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(512, 263, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (quantizer): QuantizeEMAReset()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('loading checkpoint from {}'.format(args.resume_pth))\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "net.eval()\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading transformer checkpoint from output_GPT_Final/pkeep_12layer_0.5/net_best_fid.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text2Motion_Transformer(\n",
       "  (trans_base): CrossCondTransBase(\n",
       "    (tok_emb): Embedding(514, 1024)\n",
       "    (cond_emb): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (pos_embedding): Embedding(51, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_embed): PositionEmbedding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (trans_head): CrossCondTransHead(\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalCrossConditionalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=1024, out_features=513, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.resume_trans is not None:\n",
    "    print ('loading transformer checkpoint from {}'.format(args.resume_trans))\n",
    "    ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "    trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
    "trans_encoder.train()\n",
    "trans_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2023-07-20 19:20:29,113 INFO --> \t Eva. Iter 0 :, FID. 0.1684, Diversity Real. 9.4626, Diversity. 10.0246, R_precision_real. [0.52025862 0.70387931 0.79719828], R_precision. [0.49051724 0.66831897 0.7700431 ], matching_score_real. 2.9668814675561315, matching_score_pred. 3.143495191376785, multimodality. 2.0688\n",
      "3\n",
      "2023-07-21 01:20:47,325 INFO --> \t Eva. Iter 0 :, FID. 0.1969, Diversity Real. 9.3390, Diversity. 9.6946, R_precision_real. [0.50948276 0.70560345 0.79612069], R_precision. [0.46939655 0.66206897 0.76336207], matching_score_real. 3.000256005648909, matching_score_pred. 3.177171958726028, multimodality. 2.0835\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "fid = []\n",
    "div = []\n",
    "top1 = []\n",
    "top2 = []\n",
    "top3 = []\n",
    "matching = []\n",
    "multi = []\n",
    "repeat_time = 10\n",
    "\n",
    "for i in range(repeat_time):\n",
    "    if i>=2:\n",
    "        print(i)\n",
    "        best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, best_multi, writer, logger = eval_trans.evaluation_transformer_test(args.out_dir, val_loader, net, trans_encoder, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, best_multi=0, clip_model=clip_model, eval_wrapper=eval_wrapper, draw=False, savegif=False, save=False, savenpy=False)    \n",
    "        fid.append(best_fid)\n",
    "        div.append(best_div)\n",
    "        top1.append(best_top1)\n",
    "        top2.append(best_top2)\n",
    "        top3.append(best_top3)\n",
    "        matching.append(best_matching)\n",
    "        multi.append(best_multi)\n",
    "\n",
    "print('final result:')\n",
    "print('fid: ', sum(fid)/repeat_time)\n",
    "print('div: ', sum(div)/repeat_time)\n",
    "print('top1: ', sum(top1)/repeat_time)\n",
    "print('top2: ', sum(top2)/repeat_time)\n",
    "print('top3: ', sum(top3)/repeat_time)\n",
    "print('matching: ', sum(matching)/repeat_time)\n",
    "print('multi: ', sum(multi)/repeat_time)\n",
    "\n",
    "fid = np.array(fid)\n",
    "div = np.array(div)\n",
    "top1 = np.array(top1)\n",
    "top2 = np.array(top2)\n",
    "top3 = np.array(top3)\n",
    "matching = np.array(matching)\n",
    "multi = np.array(multi)\n",
    "msg_final = f\"FID. {np.mean(fid):.3f}, conf. {np.std(fid)*1.96/np.sqrt(repeat_time):.3f}, Diversity. {np.mean(div):.3f}, conf. {np.std(div)*1.96/np.sqrt(repeat_time):.3f}, TOP1. {np.mean(top1):.3f}, conf. {np.std(top1)*1.96/np.sqrt(repeat_time):.3f}, TOP2. {np.mean(top2):.3f}, conf. {np.std(top2)*1.96/np.sqrt(repeat_time):.3f}, TOP3. {np.mean(top3):.3f}, conf. {np.std(top3)*1.96/np.sqrt(repeat_time):.3f}, Matching. {np.mean(matching):.3f}, conf. {np.std(matching)*1.96/np.sqrt(repeat_time):.3f}, Multi. {np.mean(multi):.3f}, conf. {np.std(multi)*1.96/np.sqrt(repeat_time):.3f}\"\n",
    "logger.info(msg_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
